import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv2D, Dropout, Input, Concatenate, BatchNormalization, \
    GlobalAveragePooling1D, GlobalAveragePooling2D, MaxPooling2D
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras import regularizers
from spektral.layers import GCNConv
from imblearn.over_sampling import SMOTE

def create_cnn_model(n_node_max=112, n_layers_conv=6, dropout=0.3, w_decay=1e-5, activation="selu"):

    model = Sequential()
    model.add(Input(shape=(n_node_max,n_node_max, 1)))

    mul_num = 0
    conv_channels = [64, 128, 256, 256]
    for num in range(n_layers_conv):
        model.add(Conv2D(conv_channels[mul_num], (3, 3), strides=1, padding='same', activation=activation,
                          kernel_regularizer=regularizers.l1_l2(w_decay, w_decay), 
                          kernel_initializer=tf.keras.initializers.LecunNormal()))
        model.add(BatchNormalization())
        
        if num % 2:
            mul_num += 1
            model.add(MaxPooling2D(2))
            model.add(Dropout(dropout))

    model.add(GlobalAveragePooling2D())
    return model

def create_gcn_model(n_layers_conv=2, conv_channels=256, n_node_max = 112, n_node_feature=134, dropout=0.3, w_decay=1e-5, activation="selu"):
    x_features = Input(shape=(n_node_max, n_node_feature))
    x_adjacency = Input(shape=(n_node_max, n_node_max))

    h_feature = Dropout(dropout)(x_features)
    for _ in range(n_layers_conv):
        h_feature = GCNConv(conv_channels, activation=activation, 
                            kernel_regularizer=regularizers.l1_l2(w_decay, w_decay), 
                            kernel_initializer=tf.keras.initializers.LecunNormal())([h_feature, x_adjacency])
        h_feature = BatchNormalization()(h_feature)
        h_feature = Dropout(dropout)(h_feature)

    # Graph-level readout
    features = GlobalAveragePooling1D()(h_feature)
    model = Model(inputs=[x_features, x_adjacency], outputs=features)

    return model

def create_representer_model(feature_channels=256, dropout=0.3, w_decay=1e-5, activation="relu"): 
    representer = Sequential(
        [
            Dense(feature_channels, activation=activation,
                        kernel_regularizer=regularizers.l1_l2(w_decay, w_decay),
                        kernel_initializer=tf.keras.initializers.LecunNormal(), input_shape=(feature_channels, )),
            Dropout(dropout),
            Dense(feature_channels, activation=activation,
                        kernel_regularizer=regularizers.l1_l2(w_decay, w_decay),
                        kernel_initializer=tf.keras.initializers.LecunNormal()),
            Dropout(dropout),
        ],
        name="representer",
    )
    
    return representer

def create_classifier_model(feature_channels=256, dropout=0.3, w_decay=1e-5, activation="relu", n_classes=2):
    classifier = Sequential(
        [
            Input(shape=(feature_channels)),
            Dense(feature_channels // 4, activation=activation,
                        kernel_regularizer=regularizers.l1_l2(w_decay, w_decay),
                        kernel_initializer=tf.keras.initializers.LecunNormal()),
            Dropout(dropout),
            Dense(n_classes, activation="softmax")
        ],
        name="classifier",
    )

    return classifier

def create_encoder_model(n_node_feature = 134, 
                          n_node_max = 112, embd_shape = 256, 
                          w_decay=1e-5, activation="relu") : 
    extract_edge_feature_model = create_cnn_model(n_node_max = n_node_max, w_decay=w_decay, activation=activation)
    extract_node_feature_model = create_gcn_model(n_node_feature = n_node_feature, n_node_max = n_node_max, w_decay=w_decay, activation=activation)

    out = Concatenate()([extract_edge_feature_model.output,extract_node_feature_model.output])
    
    out = tf.keras.layers.Dense(embd_shape, activation=None)(out) # No activation on final dense layer
    out = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(out) # L2 normalize embeddings

    model = Model(inputs=[extract_edge_feature_model.input, extract_node_feature_model.input], outputs=out, name="Encoder_Model")

    return model

class VulDetectModel(Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Define model
        self.encoder = create_encoder_model(activation="relu")
        self.representer = create_representer_model(activation="relu") 
        self.classifier = create_classifier_model(activation="relu", n_classes=2, feature_channels=256)
        self.resampler = SMOTE(random_state=1000, sampling_strategy=0.75)
        self.build(input_shape=[(None, 112, 112, 1), (None, 112, 134), (None, 112, 112)])

    def compile(self, c_optimizer, r_optimizer, e_optimizer, contrastive_loss_r_fn=None, contrastive_loss_e_fn=None, **kwargs):
        super().compile(**kwargs)
        self.e_optimizer = e_optimizer
        self.r_optimizer = r_optimizer
        self.c_optimizer = c_optimizer
        self.contrastive_loss_e_fn = contrastive_loss_e_fn
        self.contrastive_loss_r_fn = contrastive_loss_r_fn

    def train_step(self, batch, **kwargs):
        if len(batch) == 3:
            inputs, target, sample_weight = batch
        else:
            sample_weight = None
            inputs, target = batch

        # ============================= Training ===================================
        # Train the classifier
        with tf.GradientTape() as e_tape:
            with tf.GradientTape() as r_tape:
                with tf.GradientTape() as c_tape:

                    # Encode inputs
                    encode_vectors = self.encoder(inputs)

                    # Resample training data
                    embed_vectors, labels = self.resampler.fit_resample(encode_vectors.numpy(), target.numpy())
                    embed_vectors, labels = tf.convert_to_tensor(embed_vectors), tf.convert_to_tensor(labels) 

                    # Representation vectors
                    embed_vectors = self.representer(embed_vectors)

                    # Classifier
                    predictions = self.classifier(embed_vectors)

                    # Calculate loss
                    class_loss = self.compiled_loss(labels, predictions, 
                                              sample_weight=None,
                                              regularization_losses=self.losses)
                    represent_loss = class_loss + self.contrastive_loss_r_fn(labels, embed_vectors, sample_weight=None)
                    encode_loss = represent_loss + self.contrastive_loss_e_fn(target, encode_vectors, sample_weight=[sample_weight])

                    # Cal gradient
                    class_grads = c_tape.gradient(class_loss, self.classifier.trainable_weights)
                    represent_grads = r_tape.gradient(represent_loss, self.representer.trainable_weights)
                    encode_grads = e_tape.gradient(encode_loss, self.encoder.trainable_weights)

                # Update weights    
                self.c_optimizer.apply_gradients(
                    zip(class_grads, self.classifier.trainable_weights)
                )
            self.r_optimizer.apply_gradients(
                zip(represent_grads, self.representer.trainable_weights)
            )
        self.e_optimizer.apply_gradients(
            zip(encode_grads, self.encoder.trainable_weights)
        )

        # ======================== Show metrics ====================================
        # Encode inputs
        encode_vectors = self.encoder(inputs)
        # Representation vectors
        embed_vectors = self.representer(encode_vectors)
        # Classifier
        predictions = self.classifier(embed_vectors)

        # Update the metrics.
        # Metrics are configured in `compile()`.
        y_pred = tf.math.argmax(predictions, axis = 1)
        self.compiled_metrics.update_state(target, y_pred, sample_weight=None)

        # Return a dict mapping metric names to current value.
        # Note that it will include the loss (tracked in self.metrics).
        losses_metric = {m.name: m.result() for m in self.metrics}
        losses_metric["r_loss"] = represent_loss
        losses_metric["e_loss"] = encode_loss
        
        return losses_metric

    def test_step(self, batch, **kwargs):
        if len(batch) == 3:
            inputs, target, sample_weight = batch
        else:
            sample_weight = None
            inputs, target = batch

        # ---- Forward -----
        # Encode inputs
        encode_vectors = self.encoder(inputs)
        # Representation vectors
        embed_vectors = self.representer(encode_vectors)
        # Classifier
        predictions = self.classifier(embed_vectors)

        # Calculate loss
        class_loss = self.compiled_loss(target, predictions, 
                                  sample_weight=None,
                                  regularization_losses=self.losses)
        represent_loss = class_loss + self.contrastive_loss_r_fn(target, embed_vectors, sample_weight=None)
        encode_loss = represent_loss + self.contrastive_loss_e_fn(target, encode_vectors, sample_weight=None)

        # Update the metrics.
        # Metrics are configured in `compile()`.
        y_pred = tf.math.argmax(predictions, axis = 1)
        self.compiled_metrics.update_state(target, y_pred, sample_weight=None)

        # # Return a dict mapping metric names to current value.
        # # Note that it will include the loss (tracked in self.metrics).
        losses_metric = {m.name: m.result() for m in self.metrics}
        losses_metric["r_loss"] = represent_loss
        losses_metric["e_loss"] = encode_loss
        
        return losses_metric
    
    def predict(self, inputs, **kwargs):
        # Encode inputs
        encode_vectors = self.encoder.predict(inputs, verbose=0)
        # Representation vectors
        embed_vectors = self.representer.predict(encode_vectors, verbose=0)
        # Classifier
        predictions = self.classifier.predict(embed_vectors)

        return encode_vectors, embed_vectors, predictions

    def call(self, inputs, **kwargs):
        # Encode inputs
        encode_vectors = self.encoder(inputs)
        # Representation vectors
        embed_vectors = self.representer(encode_vectors)
        # Classifier
        predictions = self.classifier(embed_vectors)

        return predictions

if __name__ == "__main__":
    # Classifier model
    model = VulDetectModel()
    model.summary()
    model.load_weights("weights/representation_model.h5")